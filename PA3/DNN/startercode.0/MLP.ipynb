{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import softmax_cross_entropy, add_momentum, data_loader_mnist, predict_label, DataSplit\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "# 1. One linear Neural Network layer with forward and backward steps\n",
    "\n",
    "### Modules ###\n",
    "\n",
    "    ########################################################################################\n",
    "    #   The following three modules (class) are what you need to complete  (check TODO)    #\n",
    "    ########################################################################################\n",
    "\n",
    "class linear_layer:\n",
    "\n",
    "    \"\"\"\n",
    "        The linear (affine/fully-connected) module.\n",
    "\n",
    "        It is built up with two arguments:\n",
    "        - input_D: the dimensionality of the input example/instance of the forward pass\n",
    "        - output_D: the dimensionality of the output example/instance of the forward pass\n",
    "\n",
    "        It has two learnable parameters:\n",
    "        - self.params['W']: the W matrix (numpy array) of shape input_D-by-output_D\n",
    "        - self.params['b']: the b vector (numpy array) of shape 1-by-output_D\n",
    "\n",
    "        It will record the partial derivatives of loss w.r.t. self.params['W'] and self.params['b'] in:\n",
    "        - self.gradient['W']: input_D-by-output_D numpy array\n",
    "        - self.gradient['b']: 1-by-output_D numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_D, output_D):\n",
    "\n",
    "        self.params = dict()\n",
    "        self.params['W'] = np.random.normal(loc=0, scale=0.1, size=(input_D, output_D))\n",
    "        self.params['b'] = np.random.normal(loc=0, scale=0.1, size=(1,output_D) )\n",
    "        ###############################################################################################\n",
    "        # TODO: Use np.random.normal() with mean as 0 and standard deviation as 0.1\n",
    "        # W Shape (input_D, output_D), b shape (1, output_D)\n",
    "        ###############################################################################################\n",
    "        raise NotImplementedError(\"Not Implemented function: __init__, class: linear_layer\")\n",
    "\n",
    "        self.gradient = dict()\n",
    "        self.gradient['W'] = np.zeros((input_D, output_D))\n",
    "        self.gradient['b'] = np.zeros((1,output_D))        \n",
    "        ###############################################################################################\n",
    "        # TODO: Initialize gradients with zeros\n",
    "        # Note: Shape of gradient is same as the respective variables\n",
    "        ###############################################################################################\n",
    "        raise NotImplementedError(\"Not Implemented function: __init__, class: linear_layer\")\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        \"\"\"\n",
    "            The forward pass of the linear (affine/fully-connected) module.\n",
    "\n",
    "            Input:\n",
    "            - X: A N-by-input_D numpy array, where each 'row' is an input example/instance (i.e., X[i], where i = 1,...,N).\n",
    "                The mini-batch size is N.\n",
    "\n",
    "            Return:\n",
    "            - forward_output: A N-by-output_D numpy array, where each 'row' is an output example/instance.\n",
    "        \"\"\"\n",
    "        forward_output = X.dot(self.params['W']) + self.params['b']\n",
    "        ################################################################################\n",
    "        # TODO: Implement the linear forward pass. Store the result in forward_output  #\n",
    "        ################################################################################\n",
    "        return forward_output\n",
    "\n",
    "    def backward(self, X, grad):\n",
    "\n",
    "        \"\"\"\n",
    "            The backward pass of the linear (affine/fully-connected) module.\n",
    "\n",
    "            Input:\n",
    "            - X: A N-by-input_D numpy array, the input to the forward pass.\n",
    "            - grad: A N-by-output_D numpy array, where each 'row' (say row i) is the partial derivatives of the mini-batch loss\n",
    "                 w.r.t. forward_output[i].\n",
    "\n",
    "            Operation:\n",
    "            - Compute the partial derivatives (gradients) of the mini-batch loss w.r.t. self.params['W'], self.params['b'].\n",
    "            \n",
    "            Return:\n",
    "            - backward_output: A N-by-input_D numpy array, where each 'row' (say row i) is the partial derivatives of the mini-batch loss w.r.t. X[i].\n",
    "        \"\"\"\n",
    "\n",
    "        #################################################################################################\n",
    "        # TODO: Implement the backward pass (i.e., compute the following three terms)\n",
    "        # self.gradient['W'] = ? (input_D-by-output_D numpy array, the gradient of the mini-batch loss w.r.t. self.params['W'])\n",
    "        # self.gradient['b'] = ? (1-by-output_D numpy array, the gradient of the mini-batch loss w.r.t. self.params['b'])\n",
    "        # backward_output = ? (N-by-input_D numpy array, the gradient of the mini-batch loss w.r.t. X)\n",
    "        # only return backward_output, but need to compute self.gradient['W'] and self.gradient['b']\n",
    "        #################################################################################################\n",
    "        self.gradient['W'] = X.T.dot(grad)\n",
    "        self.gradient['b'] = np.sum(grad, axis=0)\n",
    "        backward_output = grad.dot(self.params['W'].T)\n",
    "        \n",
    "        raise NotImplementedError(\"Not Implemented function: backward, class: linear_layer\")\n",
    "        return backward_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14285937, -0.00830946],\n",
       "       [ 0.15571746,  0.00513428],\n",
       "       [-0.02342817, -0.03687462],\n",
       "       [-0.03550595, -0.02705871],\n",
       "       [-0.05976301, -0.16908489]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(loc=0, scale=0.1, size=(5, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This script is adapted and modified based on the assignment of\n",
    "\n",
    "Do not change this script.\n",
    "If our script cannot run your code or the format is improper, your code will not be graded.\n",
    "\"\"\"\n",
    "\n",
    "# Softmax loss and Softmax gradient\n",
    "### Loss functions ###\n",
    "\n",
    "class softmax_cross_entropy:\n",
    "    def __init__(self):\n",
    "        self.expand_Y = None\n",
    "        self.calib_logit = None\n",
    "        self.sum_exp_calib_logit = None\n",
    "        self.prob = None\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        self.expand_Y = np.zeros(X.shape).reshape(-1)\n",
    "        self.expand_Y[Y.astype(int).reshape(-1) + np.arange(X.shape[0]) * X.shape[1]] = 1.0\n",
    "        self.expand_Y = self.expand_Y.reshape(X.shape)\n",
    "\n",
    "        self.calib_logit = X - np.amax(X, axis = 1, keepdims = True)\n",
    "        self.sum_exp_calib_logit = np.sum(np.exp(self.calib_logit), axis = 1, keepdims = True)\n",
    "        self.prob = np.exp(self.calib_logit) / self.sum_exp_calib_logit\n",
    "\n",
    "        forward_output = - np.sum(np.multiply(self.expand_Y, self.calib_logit - np.log(self.sum_exp_calib_logit))) / X.shape[0]\n",
    "        return forward_output\n",
    "\n",
    "    def backward(self, X, Y):\n",
    "        backward_output = - (self.expand_Y - self.prob) / X.shape[0]\n",
    "        return backward_output\n",
    "\n",
    "\n",
    "### Momentum ###\n",
    "\n",
    "def add_momentum(model):\n",
    "    momentum = dict()\n",
    "    for module_name, module in model.items():\n",
    "        if hasattr(module, 'params'):\n",
    "            for key, _ in module.params.items():\n",
    "                momentum[module_name + '_' + key] = np.zeros(module.gradient[key].shape)\n",
    "    return momentum\n",
    "\n",
    "\n",
    "def data_loader_mnist(dataset):\n",
    "    # This function reads the MNIST data and separate it into train, val, and test set\n",
    "    with open(dataset, 'r') as f:\n",
    "        data_set = json.load(f)\n",
    "    train_set, valid_set, test_set = data_set['train'], data_set['valid'], data_set['test']\n",
    "\n",
    "    Xtrain = np.array(train_set[0])\n",
    "    Ytrain = np.array(train_set[1])\n",
    "    Xvalid = np.array(valid_set[0])\n",
    "    Yvalid = np.array(valid_set[1])\n",
    "    Xtest = np.array(test_set[0])\n",
    "    Ytest = np.array(test_set[1])\n",
    "\n",
    "    return Xtrain, Ytrain, Xvalid, Yvalid, Xtest, Ytest\n",
    "\n",
    "\n",
    "def predict_label(f):\n",
    "    # This is a function to determine the predicted label given scores\n",
    "    if f.shape[1] == 1:\n",
    "        return (f > 0).astype(float)\n",
    "    else:\n",
    "        return np.argmax(f, axis=1).astype(float).reshape((f.shape[0], -1))\n",
    "\n",
    "\n",
    "class DataSplit:\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.N, self.d = self.X.shape\n",
    "\n",
    "    def get_example(self, idx):\n",
    "        batchX = np.zeros((len(idx), self.d))\n",
    "        batchY = np.zeros((len(idx), 1))\n",
    "        for i in range(len(idx)):\n",
    "            batchX[i] = self.X[idx[i]]\n",
    "            batchY[i, :] = self.Y[idx[i]]\n",
    "        return batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ReLU Activation\n",
    "\n",
    "\n",
    "class relu:\n",
    "\n",
    "    \"\"\"\n",
    "        The relu (rectified linear unit) module.\n",
    "\n",
    "        It is built up with NO arguments.\n",
    "        It has no parameters to learn.\n",
    "        self.mask is an attribute of relu. You can use it to store things (computed in the forward pass) for the use in the backward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        \"\"\"\n",
    "            The forward pass of the relu (rectified linear unit) module.\n",
    "\n",
    "            Input:\n",
    "            - X: A numpy array of arbitrary shape.\n",
    "            \n",
    "            Return:\n",
    "            - forward_output: A numpy array of the same shape of X\n",
    "        \"\"\"\n",
    "        self.mask = (X > 0).astype(int)\n",
    "        forward_output = np.maximum(0, X)\n",
    "        ################################################################################\n",
    "        # TODO: Implement the relu forward pass. Store the result in forward_output    #\n",
    "        ################################################################################\n",
    "        raise NotImplementedError(\"Not Implemented function: forward, class: relu\")\n",
    "        return forward_output\n",
    "\n",
    "    def backward(self, X, grad):\n",
    "\n",
    "        \"\"\"\n",
    "            The backward pass of the relu (rectified linear unit) module.\n",
    "\n",
    "            Input:\n",
    "            - X: A numpy array of arbitrary shape, the input to the forward pass.\n",
    "            - grad: A numpy array of the same shape of X, where each element is the partial derivative of the mini-batch loss\n",
    "                 w.r.t. the corresponding element in forward_output.\n",
    "\n",
    "            Return:\n",
    "            - backward_output: A numpy array of the same shape as X, where each element is the partial derivative of the mini-batch loss w.r.t. \n",
    "            the corresponding element in  X.\n",
    "        \"\"\"\n",
    "        backward_output = np.multiply(  grad , self.mask )\n",
    "\n",
    "        ####################################################################################################\n",
    "        # TODO: Implement the backward pass\n",
    "        # You can use the mask created in the forward step.\n",
    "        ####################################################################################################\n",
    "        raise NotImplementedError(\"Not Implemented function: backward, class: relu\")\n",
    "        return backward_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1],\n",
       "       [1, 0, 1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = np.array([[-1,1,3], [2,-5,3]])\n",
    "(aa > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. tanh Activation\n",
    "\n",
    "class tanh:\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        \"\"\"\n",
    "            Input:\n",
    "            - X: A numpy array of arbitrary shape.\n",
    "\n",
    "            Return:\n",
    "            - forward_output: A numpy array of the same shape of X\n",
    "        \"\"\"\n",
    "        forward_output = np.tanh(X)\n",
    "\n",
    "        ################################################################################\n",
    "        # TODO: Implement the tanh forward pass. Store the result in forward_output\n",
    "        # You can use np.tanh()\n",
    "        ################################################################################\n",
    "        raise NotImplementedError(\"Not Implemented function: forward, class: tanh\")\n",
    "        return forward_output\n",
    "\n",
    "    def backward(self, X, grad):\n",
    "\n",
    "        \"\"\"\n",
    "            Input:\n",
    "            - X: A numpy array of arbitrary shape, the input to the forward pass.\n",
    "            - grad: A numpy array of the same shape of X, where each element is the partial derivative of the mini-batch \n",
    "            loss w.r.t. the corresponding element in forward_output.\n",
    "\n",
    "            Return:\n",
    "            - backward_output: A numpy array of the same shape as X, where each element is the partial derivative of the mini-batch \n",
    "            loss w.r.t. the corresponding element in  X.\n",
    "        \"\"\"\n",
    "\n",
    "        ####################################################################################################\n",
    "        # TODO: Implement the backward pass\n",
    "        # Derivative of tanh is (1 - tanh^2)\n",
    "        ####################################################################################################\n",
    "        backward_output = np.multiply(  (1 - np.power(np.tanh(X), 2)), grad )\n",
    "        raise NotImplementedError(\"Not Implemented function: backward, class: tanh\")\n",
    "        return backward_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4621171572600098"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 / (1 + np.exp(-2 * 0.5) ) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46211715726000974"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tanh(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Dropout\n",
    "\n",
    "class dropout:\n",
    "\n",
    "    \"\"\"\n",
    "        It is built up with one arguments:\n",
    "        - r: the dropout rate\n",
    "\n",
    "        It has no parameters to learn.\n",
    "        self.mask is an attribute of dropout. You can use it to store things (computed in the forward pass) for the use in the backward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, r):\n",
    "        self.r = r\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, X, is_train):\n",
    "\n",
    "        \"\"\"\n",
    "            Input:\n",
    "            - X: A numpy array of arbitrary shape.\n",
    "            - is_train: A boolean value. If False, no dropout is performed.\n",
    "\n",
    "            Operation:\n",
    "            - If p >= self.r, output that element multiplied by (1.0 / (1 - self.r)); otherwise, output 0 for that element\n",
    "            \n",
    "            Return:\n",
    "            - forward_output: A numpy array of the same shape of X (the output of dropout)\n",
    "        \"\"\"\n",
    "\n",
    "        ################################################################################\n",
    "        #  TODO: We provide the forward pass to you. You only need to understand it.   #\n",
    "        ################################################################################\n",
    "\n",
    "        if is_train:\n",
    "            self.mask = (np.random.uniform(0.0, 1.0, X.shape) >= self.r).astype(float) * (1.0 / (1.0 - self.r))\n",
    "        else:\n",
    "            self.mask = np.ones(X.shape)\n",
    "        forward_output = np.multiply(X, self.mask)\n",
    "        return forward_output\n",
    "\n",
    "    def backward(self, X, grad):\n",
    "\n",
    "        \"\"\"\n",
    "            Input:\n",
    "            - X: A numpy array of arbitrary shape, the input to the forward pass.\n",
    "            - grad: A numpy array of the same shape of X, where each element is the partial derivative of the mini-batch loss w.r.t. \n",
    "            the corresponding element in forward_output.\n",
    "\n",
    "\n",
    "            Return:\n",
    "            - backward_output: A numpy array of the same shape as X, where each element is the partial derivative of the mini-batch loss \n",
    "            w.r.t. the corresponding element in X.\n",
    "        \"\"\"\n",
    "\n",
    "        ####################################################################################################\n",
    "        # TODO: Implement the backward pass\n",
    "        # You can use the mask created in the forward step\n",
    "        ####################################################################################################\n",
    "        backward_output = np.multiply(grad, self.mask)\n",
    "        raise NotImplementedError(\"Not Implemented function: backward, class: dropout\")\n",
    "        return backward_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Mini-batch Gradient Descent Optimization\n",
    "\n",
    "\n",
    "def miniBatchStochasticGradientDescent(model, momentum, _lambda, _alpha, _learning_rate):\n",
    "\n",
    "    '''\n",
    "        Input:\n",
    "            model: Dictionary containing all parameters of the model\n",
    "            momentum: Check add_momentum() function in utils.py to understand this parameter\n",
    "            _lambda: Regularization constant\n",
    "            _alpha: Momentum hyperparameter\n",
    "            _learning_rate: Learning rate for the update\n",
    "\n",
    "        Note: You can learn more about momentum here: https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/\n",
    "\n",
    "        Returns: Updated model\n",
    "    '''\n",
    "\n",
    "\n",
    "    for module_name, module in model.items():\n",
    "\n",
    "        # check if a module has learnable parameters\n",
    "        if hasattr(module, 'params'):\n",
    "            for key, _ in module.params.items():\n",
    "                g = module.gradient[key] + _lambda * module.params[key]\n",
    "\n",
    "                if _alpha > 0.0:\n",
    "\n",
    "                    #################################################################################\n",
    "                    # TODO: Update momentun using the formula:\n",
    "                    # m = alpha * m - learning_rate * g (Check add_momentum() function in utils file)\n",
    "                    # And update model parameter\n",
    "                    #################################################################################\n",
    "                    momentum[module_name + '_' + key] = _alpha * momentum[module_name + '_' + key] - _learning_rate * g\n",
    "                    module.params[key] += momentum[module_name + '_' + key]\n",
    "                    raise NotImplementedError(\"Not Implemented function: miniBatchGradientDescent\")                    \n",
    "                else:\n",
    "                    module.params[key] -= _learning_rate * g\n",
    "                    #################################################################################\n",
    "                    # TODO: update model parameter without momentum\n",
    "                    #################################################################################\n",
    "                    raise NotImplementedError(\"Not Implemented function: miniBatchGradientDescent\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(main_params, optimization_type=\"minibatch_sgd\"):\n",
    "\n",
    "    ### set the random seed ###\n",
    "    np.random.seed(int(main_params['random_seed']))\n",
    "\n",
    "    ### data processing ###\n",
    "    Xtrain, Ytrain, Xval, Yval , _, _ = data_loader_mnist(dataset = main_params['input_file'])\n",
    "    N_train, d = Xtrain.shape\n",
    "    N_val, _ = Xval.shape\n",
    "\n",
    "    index = np.arange(10)\n",
    "    unique, counts = np.unique(Ytrain, return_counts=True)\n",
    "    counts = dict(zip(unique, counts)).values()\n",
    "\n",
    "    trainSet = DataSplit(Xtrain, Ytrain)\n",
    "    valSet = DataSplit(Xval, Yval)\n",
    "\n",
    "    ### building/defining MLP ###\n",
    "    \"\"\"\n",
    "    In this script, we are going to build a MLP for a 10-class classification problem on MNIST.\n",
    "    The network structure is input --> linear --> relu --> dropout --> linear --> softmax_cross_entropy loss\n",
    "    the hidden_layer size (num_L1) is 1000\n",
    "    the output_layer size (num_L2) is 10\n",
    "    \"\"\"\n",
    "    model = dict()\n",
    "    num_L1 = 1000\n",
    "    num_L2 = 10\n",
    "\n",
    "    # experimental setup\n",
    "    num_epoch = int(main_params['num_epoch'])\n",
    "    minibatch_size = int(main_params['minibatch_size'])\n",
    "\n",
    "    # optimization setting: _alpha for momentum, _lambda for weight decay\n",
    "    _learning_rate = float(main_params['learning_rate'])\n",
    "    _step = 10\n",
    "    _alpha = float(main_params['alpha'])\n",
    "    _lambda = float(main_params['lambda'])\n",
    "    _dropout_rate = float(main_params['dropout_rate'])\n",
    "    _activation = main_params['activation']\n",
    "\n",
    "\n",
    "    if _activation == 'relu':\n",
    "        act = relu\n",
    "    else:\n",
    "        act = tanh\n",
    "\n",
    "    # create objects (modules) from the module classes\n",
    "    model['L1'] = linear_layer(input_D = d, output_D = num_L1)\n",
    "    model['nonlinear1'] = act()\n",
    "    model['drop1'] = dropout(r = _dropout_rate)\n",
    "    model['L2'] = linear_layer(input_D = num_L1, output_D = num_L2)\n",
    "    model['loss'] = softmax_cross_entropy()\n",
    "\n",
    "    # Momentum\n",
    "    if _alpha > 0.0:\n",
    "        momentum = add_momentum(model)\n",
    "    else:\n",
    "        momentum = None\n",
    "\n",
    "    train_acc_record = []\n",
    "    val_acc_record = []\n",
    "\n",
    "    train_loss_record = []\n",
    "    val_loss_record = []\n",
    "\n",
    "    ### run training and validation ###\n",
    "    for t in range(num_epoch):\n",
    "        print('At epoch ' + str(t + 1))\n",
    "        if (t % _step == 0) and (t != 0):\n",
    "            _learning_rate = _learning_rate * 0.1\n",
    "\n",
    "        idx_order = np.random.permutation(N_train)\n",
    "\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "        train_count = 0\n",
    "\n",
    "        val_acc = 0.0\n",
    "        val_count = 0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        for i in range(int(np.floor(N_train / minibatch_size))):\n",
    "\n",
    "            # get a mini-batch of data\n",
    "            x, y = trainSet.get_example(idx_order[i * minibatch_size : (i + 1) * minibatch_size])\n",
    "\n",
    "            ### forward ###\n",
    "            a1 = model['L1'].forward(x)\n",
    "            h1 = model['nonlinear1'].forward(a1)\n",
    "            d1 = model['drop1'].forward(h1, is_train = True)\n",
    "            a2 = model['L2'].forward(d1)\n",
    "            loss = model['loss'].forward(a2, y)\n",
    "\n",
    "\n",
    "            ### backward ###\n",
    "            grad_a2 = model['loss'].backward(a2, y)\n",
    "            ######################################################################################\n",
    "            # TODO: Call the backward methods of every layer in the model in reverse order\n",
    "            # We have given the first and last backward calls\n",
    "            # Do not modify them.\n",
    "            ######################################################################################\n",
    "            grad_d1 = model['L2'].backward(d1, grad_a2)\n",
    "            grad_h1 = model['drop1'].backward(h1, grad_d1)\n",
    "            grad_a1 = model['nonlinear1'].backward(a1, grad_h1)           \n",
    "            raise NotImplementedError(\"Not Implemented BACKWARD PASS in main()\")\n",
    "\n",
    "            ######################################################################################\n",
    "            # NOTE: DO NOT MODIFY CODE BELOW THIS, until next TODO\n",
    "            ######################################################################################\n",
    "            grad_x = model['L1'].backward(x, grad_a1)\n",
    "\n",
    "            ### gradient_update ###\n",
    "            model = miniBatchStochasticGradientDescent(model, momentum, _lambda, _alpha, _learning_rate)\n",
    "            \n",
    "        ### Computing training accuracy and obj ###\n",
    "        for i in range(int(np.floor(N_train / minibatch_size))):\n",
    "\n",
    "            x, y = trainSet.get_example(np.arange(i * minibatch_size, (i + 1) * minibatch_size))\n",
    "\n",
    "            ### forward ###\n",
    "            ######################################################################################\n",
    "            # TODO: Call the forward methods of every layer in the model in order\n",
    "            # Check above forward code\n",
    "            # Make sure to keep train as False\n",
    "            ######################################################################################\n",
    "            a1 = model['L1'].forward(x)\n",
    "            h1 = model['nonlinear1'].forward(a1)\n",
    "            d1 = model['drop1'].forward(h1, is_train = True)\n",
    "            a2 = model['L2'].forward(d1)\n",
    "            \n",
    "            raise NotImplementedError(\"Not Implemented COMPUTING TRAINING ACCURACY in main()\")\n",
    "\n",
    "            ######################################################################################\n",
    "            # NOTE: DO NOT MODIFY CODE BELOW THIS, until next TODO\n",
    "            ######################################################################################\n",
    "\n",
    "            loss = model['loss'].forward(a2, y)\n",
    "            train_loss += loss\n",
    "            train_acc += np.sum(predict_label(a2) == y)\n",
    "            train_count += len(y)\n",
    "\n",
    "        train_loss = train_loss\n",
    "        train_acc = train_acc / train_count\n",
    "        train_acc_record.append(train_acc)\n",
    "        train_loss_record.append(train_loss)\n",
    "\n",
    "        print('Training loss at epoch ' + str(t + 1) + ' is ' + str(train_loss))\n",
    "        print('Training accuracy at epoch ' + str(t + 1) + ' is ' + str(train_acc))\n",
    "\n",
    "        ### Computing validation accuracy ###\n",
    "        for i in range(int(np.floor(N_val / minibatch_size))):\n",
    "\n",
    "            x, y = valSet.get_example(np.arange(i * minibatch_size, (i + 1) * minibatch_size))\n",
    "\n",
    "            ### forward ###\n",
    "            ######################################################################################\n",
    "            # TODO: Call the forward methods of every layer in the model in order\n",
    "            # Check above forward code\n",
    "            # Make sure to keep train as False\n",
    "            ######################################################################################\n",
    "            a1 = model['L1'].forward(x)\n",
    "            h1 = model['nonlinear1'].forward(a1)\n",
    "            d1 = model['drop1'].forward(h1, is_train = True)\n",
    "            a2 = model['L2'].forward(d1)\n",
    "            \n",
    "            raise NotImplementedError(\"Not Implemented COMPUTING VALIDATION ACCURACY in main()\")\n",
    "            \n",
    "            ######################################################################################\n",
    "            # NOTE: DO NOT MODIFY CODE BELOW THIS, until next TODO\n",
    "            ######################################################################################\n",
    "\n",
    "            loss = model['loss'].forward(a2, y)\n",
    "            val_loss += loss\n",
    "            val_acc += np.sum(predict_label(a2) == y)\n",
    "            val_count += len(y)\n",
    "\n",
    "        val_loss_record.append(val_loss)\n",
    "        val_acc = val_acc / val_count\n",
    "        val_acc_record.append(val_acc)\n",
    "\n",
    "        print('Validation accuracy at epoch ' + str(t + 1) + ' is ' + str(val_acc))\n",
    "\n",
    "    # save file\n",
    "    json.dump({'train': train_acc_record, 'val': val_acc_record},\n",
    "              open('MLP_lr' + str(main_params['learning_rate']) +\n",
    "                   '_m' + str(main_params['alpha']) +\n",
    "                   '_w' + str(main_params['lambda']) +\n",
    "                   '_d' + str(main_params['dropout_rate']) +\n",
    "                   '_a' + str(main_params['activation']) +\n",
    "                   '.json', 'w'))\n",
    "\n",
    "    print('Finish running!')\n",
    "    return train_loss_record, val_loss_record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--random_seed RANDOM_SEED]\n",
      "                             [--learning_rate LEARNING_RATE] [--alpha ALPHA]\n",
      "                             [--lambda LAMBDA] [--dropout_rate DROPOUT_RATE]\n",
      "                             [--num_epoch NUM_EPOCH]\n",
      "                             [--minibatch_size MINIBATCH_SIZE]\n",
      "                             [--activation ACTIVATION]\n",
      "                             [--input_file INPUT_FILE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\admin\\AppData\\Roaming\\jupyter\\runtime\\kernel-a952bde5-749e-45b2-9776-2df5eaba642c.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from utils import softmax_cross_entropy, add_momentum, data_loader_mnist, predict_label, DataSplit\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import json\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--random_seed', default=42)\n",
    "parser.add_argument('--learning_rate', default=0.01)\n",
    "parser.add_argument('--alpha', default=0.0)\n",
    "parser.add_argument('--lambda', default=0.0)\n",
    "parser.add_argument('--dropout_rate', default=0.0)\n",
    "parser.add_argument('--num_epoch', default=10)\n",
    "parser.add_argument('--minibatch_size', default=5)\n",
    "parser.add_argument('--activation', default='relu')\n",
    "parser.add_argument('--input_file', default='mnist_subset.json')\n",
    "args = parser.parse_args()\n",
    "main_params = vars(args)\n",
    "main(main_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
