{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f1_score(real_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Information on F1 score - https://en.wikipedia.org/wiki/F1_score\n",
    "    :param real_labels: List[int]\n",
    "    :param predicted_labels: List[int]\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    assert len(real_labels) == len(predicted_labels)\n",
    "#     raise NotImplementedError\n",
    "    TP = FP = FN = 0\n",
    "    for i in range(len(real_labels)):\n",
    "        x = real_labels[i]\n",
    "        y = predicted_labels[i]\n",
    "        if x == 1 and y == 1:\n",
    "            TP += 1\n",
    "        elif x == 0 and y == 1:\n",
    "            FP += 1\n",
    "        elif x == 1 and y == 0:\n",
    "            FN += 1\n",
    "     \n",
    "    if 2 * TP + FP + FN == 0:\n",
    "        return 0\n",
    "    \n",
    "    res = 2 * TP / float(2 * TP + FP + FN)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "class Distances:\n",
    "    @staticmethod\n",
    "    # TODO\n",
    "    def minkowski_distance(point1, point2):\n",
    "        \"\"\"\n",
    "        Minkowski distance is the generalized version of Euclidean Distance\n",
    "        It is also know as L-p norm (where p>=1) that you have studied in class\n",
    "        For our assignment we need to take p=3\n",
    "        Information on Minkowski distance - https://en.wikipedia.org/wiki/Minkowski_distance\n",
    "        :param point1: List[float]\n",
    "        :param point2: List[float]\n",
    "        :return: float\n",
    "        \"\"\"\n",
    "#         raise NotImplementedError\n",
    "        return np.power(sum( [pow(abs(x - y), 3) for x, y in zip(point1, point2)]), 1/3)\n",
    "        \n",
    "    @staticmethod\n",
    "    # TODO\n",
    "    def euclidean_distance(point1, point2):\n",
    "        \"\"\"\n",
    "        :param point1: List[float]\n",
    "        :param point2: List[float]\n",
    "        :return: float\n",
    "        \"\"\"\n",
    "#         raise NotImplementedError\n",
    "        return np.sqrt(sum( [(x - y)**2 for x, y in zip(point1, point2)]))\n",
    "\n",
    "    @staticmethod\n",
    "    # TODO\n",
    "    def inner_product_distance(point1, point2):\n",
    "        \"\"\"\n",
    "        :param point1: List[float]\n",
    "        :param point2: List[float]\n",
    "        :return: float\n",
    "        \"\"\"\n",
    "#         raise NotImplementedError\n",
    "        return sum([x*y for x, y in zip(point1, point2)])\n",
    "\n",
    "    @staticmethod\n",
    "    # TODO\n",
    "    def cosine_similarity_distance(point1, point2):\n",
    "        \"\"\"\n",
    "       :param point1: List[float]\n",
    "       :param point2: List[float]\n",
    "       :return: float\n",
    "       \"\"\"\n",
    "#         raise NotImplementedError\n",
    "        return 1 - sum([x*y for x, y in zip(point1, point2)]) / np.sqrt(sum([x**2 for x in point1]) * sum([x**2 for x in point2]))\n",
    "\n",
    "    @staticmethod\n",
    "    # TODO\n",
    "    def gaussian_kernel_distance(point1, point2):\n",
    "        \"\"\"\n",
    "       :param point1: List[float]\n",
    "       :param point2: List[float]\n",
    "       :return: float\n",
    "       \"\"\"\n",
    "#         raise NotImplementedError\n",
    "        tmp = [(x - y)**2 for x, y in zip(point1, point2)]\n",
    "        return -np.exp(-0.5 * sum(tmp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def data_processing():\n",
    "    data = pd.read_csv('heart_disease.csv', low_memory=False, sep=',', na_values='?').values\n",
    "\n",
    "    N = data.shape[0]\n",
    "\n",
    "#     np.random.shuffle(data)\n",
    "    # prepare data\n",
    "\n",
    "    ntr = int(np.round(N * 0.8))\n",
    "    nval = int(np.round(N * 0.15))\n",
    "    ntest = N - ntr - nval\n",
    "\n",
    "    # spliting training, validation, and test\n",
    "    x_train = np.append([np.ones(ntr)], data[:ntr].T[:-1], axis=0).T\n",
    "    y_train = data[:ntr].T[-1].T\n",
    "    x_val = np.append([np.ones(nval)], data[ntr:ntr + nval].T[:-1], axis=0).T\n",
    "    y_val = data[ntr:ntr + nval].T[-1].T\n",
    "    x_test = np.append([np.ones(ntest)], data[-ntest:].T[:-1], axis=0).T\n",
    "    y_test = data[-ntest:].T[-1].T\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k, distance_function):\n",
    "        \"\"\"\n",
    "        :param k: int\n",
    "        :param distance_function\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.distance_function = distance_function\n",
    "\n",
    "    # TODO: save features and lable to self\n",
    "    def train(self, features, labels):\n",
    "        \"\"\"\n",
    "        In this function, features is simply training data which is a 2D list with float values.\n",
    "        For example, if the data looks like the following: Student 1 with features age 25, grade 3.8 and labeled as 0,\n",
    "        Student 2 with features age 22, grade 3.0 and labeled as 1, then the feature data would be\n",
    "        [ [25.0, 3.8], [22.0,3.0] ] and the corresponding label would be [0,1]\n",
    "\n",
    "        For KNN, the training process is just loading of training data. Thus, all you need to do in this function\n",
    "        is create some local variable in KNN class to store this data so you can use the data in later process.\n",
    "        :param features: List[List[float]]\n",
    "        :param labels: List[int]\n",
    "        \"\"\"\n",
    "#         raise NotImplementedError\n",
    "        self.features = np.array(features)\n",
    "        self.labels = np.array(labels)\n",
    "        \n",
    "        \n",
    "    # TODO: predict labels of a list of points\n",
    "    def predict(self, features):\n",
    "        \"\"\"\n",
    "        This function takes 2D list of test data points, similar to those from train function. Here, you need process\n",
    "        every test data point, reuse the get_k_neighbours function to find the nearest k neighbours for each test\n",
    "        data point, find the majority of labels for these neighbours as the predict label for that testing data point.\n",
    "        Thus, you will get N predicted label for N test data point.\n",
    "        This function need to return a list of predicted labels for all test data points.\n",
    "        :param features: List[List[float]]\n",
    "        :return: List[int]\n",
    "        \"\"\"\n",
    "#         raise NotImplementedError\n",
    "        res = []\n",
    "        for test in features:\n",
    "            k_labels = self.get_k_neighbors(test)\n",
    "#             print(k_labels)\n",
    "            # using Counter to count each label and then use most_common to find majority\n",
    "            c = Counter(k_labels)\n",
    "            label, cnt = c.most_common(1)[0]\n",
    "            res.append(label)\n",
    "        return res\n",
    "        \n",
    "        \n",
    "    # TODO: find KNN of one point\n",
    "    def get_k_neighbors(self, point):\n",
    "        \"\"\"\n",
    "        This function takes one single data point and finds k-nearest neighbours in the training set.\n",
    "        You already have your k value, distance function and you just stored all training data in KNN class with the\n",
    "        train function. This function needs to return a list of labels of all k neighours.\n",
    "        :param point: List[float]\n",
    "        :return:  List[int]\n",
    "        \"\"\"\n",
    "#         raise NotImplementedError\n",
    "        distances = [self.distance_function(point, train) for train in self.features]\n",
    "        distances = np.array(distances)\n",
    "        # using np.argpartition to get indexes of top K min elements, we can use indexes to search labels.\n",
    "        \n",
    "        kmin_index = np.argsort(distances)[: self.k]\n",
    "#         kmin_index = np.argpartition(distances, self.k)[: self.k]\n",
    "\n",
    "        return self.labels[kmin_index]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNN(2, Distances.euclidean_distance)\n",
    "train = [[1,1,1],[2,2,2],[3,3,3],[4,4,4]]\n",
    "labels = [1,1,2,2]\n",
    "knn.train(train, labels)\n",
    "knn.predict([[1.5,3.5,1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "class HyperparameterTuner:\n",
    "    def __init__(self):\n",
    "        self.best_k = 0\n",
    "        self.f1_score_best = -1\n",
    "        self.best_distance_function = None\n",
    "        self.best_scaler = None\n",
    "        self.best_model = None\n",
    "        \n",
    "        self.sort_dict = {\n",
    "            'euclidean': 5,\n",
    "            'minkowski': 4,\n",
    "            'gaussian': 3,\n",
    "            'inner_prod': 2,\n",
    "            'cosine_dist': 1,\n",
    "            \n",
    "            'min_max_scale': 10,\n",
    "            'normalize': 9\n",
    "        }\n",
    "\n",
    "    # TODO: find parameters with the best f1 score on validation dataset\n",
    "    def tuning_without_scaling(self, distance_funcs, x_train, y_train, x_val, y_val):\n",
    "        \"\"\"\n",
    "        In this part, you should try different distance function you implemented in part 1.1, and find the best k.\n",
    "        Use k range from 1 to 30 and increment by 2. Use f1-score to compare different models.\n",
    "\n",
    "        :param distance_funcs: dictionary of distance functions you must use to calculate the distance.\n",
    "            Make sure you loop over all distance functions for each data point and each k value.\n",
    "            You can refer to test.py file to see the format in which these functions will be\n",
    "            passed by the grading script\n",
    "        :param x_train: List[List[int]] training data set to train your KNN model\n",
    "        :param y_train: List[int] train labels to train your KNN model\n",
    "        :param x_val:  List[List[int]] Validation data set will be used on your KNN predict function to produce\n",
    "            predicted labels and tune k and distance function.\n",
    "        :param y_val: List[int] validation labels\n",
    "\n",
    "        Find(tune) best k, distance_function and model (an instance of KNN) and assign to self.best_k,\n",
    "        self.best_distance_function and self.best_model respectively.\n",
    "        NOTE: self.best_scaler will be None\n",
    "\n",
    "        NOTE: When there is a tie, choose model based on the following priorities:\n",
    "        Then check distance function  [euclidean > minkowski > gaussian > inner_prod > cosine_dist]\n",
    "        If they have same distance fuction, choose model which has a less k.\n",
    "        \"\"\"\n",
    "        # You need to assign the final values to these variables\n",
    "#         self.best_k = None\n",
    "#         self.best_distance_function = None\n",
    "#         self.best_model = None\n",
    "#         raise NotImplementedError\n",
    "        \n",
    "        sort_dict = self.sort_dict\n",
    "        \n",
    "        for name, func in distance_funcs.items():\n",
    "            f1_score_best = -1\n",
    "            for k in range(1, 30, 2):\n",
    "                model = KNN(k, func)\n",
    "                model.train(x_train, y_train)\n",
    "#                 model.train(x_train[1:], y_train)\n",
    "                val_f1_score = f1_score(y_val, model.predict(x_val))\n",
    "                if val_f1_score > f1_score_best:\n",
    "                    f1_score_best = val_f1_score\n",
    "                    self.best_k = k\n",
    "                    self.best_distance_function = name\n",
    "                    self.best_model = model\n",
    "                # process tie\n",
    "                if val_f1_score == val_f1_score:\n",
    "                    if self.best_distance_function == name:\n",
    "                        if k < self.best_k:\n",
    "                            self.best_k = k\n",
    "                    else:\n",
    "                        if sort_dict[name] > sort_dict[self.best_distance_function]:\n",
    "                            self.best_distance_function = name\n",
    "         \n",
    "                        \n",
    "\n",
    "    # TODO: find parameters with the best f1 score on validation dataset, with normalized data\n",
    "    def tuning_with_scaling(self, distance_funcs, scaling_classes, x_train, y_train, x_val, y_val):\n",
    "        \"\"\"\n",
    "        This part is similar to Part 1.3 except that before passing your training and validation data to KNN model to\n",
    "        tune k and disrance function, you need to create the normalized data using these two scalers to transform your\n",
    "        data, both training and validation. Again, we will use f1-score to compare different models.\n",
    "        Here we have 3 hyperparameters i.e. k, distance_function and scaler.\n",
    "\n",
    "        :param distance_funcs: dictionary of distance funtions you use to calculate the distance. Make sure you\n",
    "            loop over all distance function for each data point and each k value.\n",
    "            You can refer to test.py file to see the format in which these functions will be\n",
    "            passed by the grading script\n",
    "        :param scaling_classes: dictionary of scalers you will use to normalized your data.\n",
    "        Refer to test.py file to check the format.\n",
    "        :param x_train: List[List[int]] training data set to train your KNN model\n",
    "        :param y_train: List[int] train labels to train your KNN model\n",
    "        :param x_val: List[List[int]] validation data set you will use on your KNN predict function to produce predicted\n",
    "            labels and tune your k, distance function and scaler.\n",
    "        :param y_val: List[int] validation labels\n",
    "\n",
    "        Find(tune) best k, distance_funtion, scaler and model (an instance of KNN) and assign to self.best_k,\n",
    "        self.best_distance_function, self.best_scaler and self.best_model respectively\n",
    "\n",
    "        NOTE: When there is a tie, choose model based on the following priorities:\n",
    "        For normalization, [min_max_scale > normalize];\n",
    "        Then check distance function  [euclidean > minkowski > gaussian > inner_prod > cosine_dist]\n",
    "        If they have same distance function, choose model which has a less k.\n",
    "        \"\"\"\n",
    "        \n",
    "        # You need to assign the final values to these variables\n",
    "#         self.best_k = None\n",
    "#         self.best_distance_function = None\n",
    "#         self.best_scaler = None\n",
    "#         self.best_model = None\n",
    "        # raise NotImplementedError\n",
    "        for scale_name, scale in scaling_classes.items():\n",
    "            scaler = scale()\n",
    "            x_train_scaled = scaler(x_train)\n",
    "            x_val_scaled = scaler(x_val)\n",
    "            sort_dict = self.sort_dict\n",
    "\n",
    "            for name, func in distance_funcs.items():\n",
    "                for k in range(1, 30, 2):\n",
    "                    model = KNN(k, func)\n",
    "                    model.train(x_train_scaled, y_train)\n",
    "                    val_f1_score = f1_score(y_val, model.predict(x_val_scaled))\n",
    "                    if val_f1_score > self.f1_score_best:\n",
    "                        self.f1_score_best = val_f1_score\n",
    "                        self.best_k = k\n",
    "                        self.best_distance_function = name\n",
    "                        self.best_model = model\n",
    "                        self.best_scaler = scale_name\n",
    "                    # process tie\n",
    "                    if val_f1_score == self.f1_score_best:\n",
    "                        if self.best_scaler == scale_name:\n",
    "                            if self.best_distance_function == func:\n",
    "                                if k < self.best_k:\n",
    "                                    self.best_k = k\n",
    "                            else:\n",
    "                                if sort_dict[name] > sort_dict[self.best_distance_function]:\n",
    "                                    self.best_distance_function = name\n",
    "                        else:\n",
    "                            if sort_dict[scale_name] > sort_dict[self.best_scaler]:\n",
    "                                self.best_scaler = scale_name\n",
    "                    \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizationScaler:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # TODO: normalize data\n",
    "    def __call__(self, features):\n",
    "        \"\"\"\n",
    "        Normalize features for every sample\n",
    "\n",
    "        Example\n",
    "        features = [[3, 4], [1, -1], [0, 0]]\n",
    "        return [[0.6, 0.8], [0.707107, -0.707107], [0, 0]]\n",
    "\n",
    "        :param features: List[List[float]]\n",
    "        :return: List[List[float]]\n",
    "        \"\"\"\n",
    "        # raise NotImplementedError\n",
    "        res = []\n",
    "        for feature in features:\n",
    "            denom = np.sqrt(sum([pow(x, 2) for x in feature]))\n",
    "            if denom == 0:\n",
    "                res.append(feature)\n",
    "            else:\n",
    "                res.append([x / denom for x in feature])\n",
    "        return res\n",
    "\n",
    "\n",
    "class MinMaxScaler:\n",
    "    \"\"\"\n",
    "    Please follow this link to know more about min max scaling\n",
    "    https://en.wikipedia.org/wiki/Feature_scaling\n",
    "    You should keep some states inside the object.\n",
    "    You can assume that the parameter of the first __call__\n",
    "    will be the training set.\n",
    "\n",
    "    Hints:\n",
    "        1. Use a variable to check for first __call__ and only compute\n",
    "            and store min/max in that case.\n",
    "\n",
    "    Note:\n",
    "        1. You may assume the parameters are valid when __call__\n",
    "            is being called the first time (you can find min and max).\n",
    "\n",
    "    Example:\n",
    "        train_features = [[0, 10], [2, 0]]\n",
    "        test_features = [[20, 1]]\n",
    "\n",
    "        scaler1 = MinMaxScale()\n",
    "        train_features_scaled = scaler1(train_features)\n",
    "        # train_features_scaled should be equal to [[0, 1], [1, 0]]\n",
    "\n",
    "        test_features_scaled = scaler1(test_features)\n",
    "        # test_features_scaled should be equal to [[10, 0.1]]\n",
    "\n",
    "        new_scaler = MinMaxScale() # creating a new scaler\n",
    "        _ = new_scaler([[1, 1], [0, 0]]) # new trainfeatures\n",
    "        test_features_scaled = new_scaler(test_features)\n",
    "        # now test_features_scaled should be [[20, 1]]\n",
    "\n",
    "    \"\"\"\n",
    "    # create instance only call __init__(), which runs only once, then call instance which will call __call__()\n",
    "    def __init__(self):\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "        self.cnt = 0\n",
    "\n",
    "    def __call__(self, features):\n",
    "        \"\"\"\n",
    "        normalize the feature vector for each sample . For example,\n",
    "        if the input features = [[2, -1], [-1, 5], [0, 0]],\n",
    "        the output should be [[1, 0], [0, 1], [0.333333, 0.16667]]\n",
    "\n",
    "        :param features: List[List[float]]\n",
    "        :return: List[List[float]]\n",
    "        \"\"\"\n",
    "        if self.cnt < 1:\n",
    "            # amin/amax return min/max of an array along the axis, so that we use broadcast to solve\n",
    "            self.min = np.amin(features, axis=0)\n",
    "            self.max = np.amax(features, axis=0)\n",
    "            self.feature = features\n",
    "            self.cnt += 1\n",
    "        \n",
    "        features = np.array(features)\n",
    "        res = (features - self.min)/(self.max - self.min)\n",
    "        return res.tolist()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape =  (242, 14)\n",
      "y_train shape =  (242,)\n",
      "**Without Scaling**\n",
      "k = 15\n",
      "distance function = cosine_dist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**With Scaling**\n",
      "k = 3\n",
      "distance function = euclidean\n",
      "scaler = min_max_scale\n"
     ]
    }
   ],
   "source": [
    "distance_funcs = {\n",
    "        'euclidean': Distances.euclidean_distance,\n",
    "        'minkowski': Distances.minkowski_distance,\n",
    "        'gaussian': Distances.gaussian_kernel_distance,\n",
    "        'inner_prod': Distances.inner_product_distance,\n",
    "        'cosine_dist': Distances.cosine_similarity_distance,\n",
    "    }\n",
    "\n",
    "scaling_classes = {\n",
    "    'min_max_scale': MinMaxScaler,\n",
    "    'normalize': NormalizationScaler,\n",
    "}\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = data_processing()\n",
    "\n",
    "print('x_train shape = ', x_train.shape)\n",
    "print('y_train shape = ', y_train.shape)\n",
    "\n",
    "tuner_without_scaling_obj = HyperparameterTuner()\n",
    "tuner_without_scaling_obj.tuning_without_scaling(distance_funcs, x_train, y_train, x_val, y_val)\n",
    "\n",
    "print(\"**Without Scaling**\")\n",
    "print(\"k =\", tuner_without_scaling_obj.best_k)\n",
    "print(\"distance function =\", tuner_without_scaling_obj.best_distance_function)\n",
    "\n",
    "tuner_with_scaling_obj = HyperparameterTuner()\n",
    "tuner_with_scaling_obj.tuning_with_scaling(distance_funcs, scaling_classes, x_train, y_train, x_val, y_val)\n",
    "\n",
    "print(\"\\n**With Scaling**\")\n",
    "print(\"k =\", tuner_with_scaling_obj.best_k)\n",
    "print(\"distance function =\", tuner_with_scaling_obj.best_distance_function)\n",
    "print(\"scaler =\", tuner_with_scaling_obj.best_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "labels = np.array([1,2,1,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112781244591328"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = []\n",
    "total = labels.size\n",
    "for label in np.unique(labels):\n",
    "    tmp.append(np.where(labels==label)[0].size)\n",
    "tmp = np.array(tmp)\n",
    "tmp = tmp/total\n",
    "S = np.sum([ -s*np.log2(s) for s in tmp])\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
